# -*- coding: utf-8 -*-
"""LfD Final Project - SVM_Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13kauHmgsU8X2tK45VaOAgqN_lnavYg8B

# IMPORTS
"""

import os
import argparse
import random

from sklearn.svm import LinearSVC
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
import string
string.punctuation
from metric_utils import calculate_scores
from data_utils import preprocessing, read_corpus, \
                       get_vectorizer
from utils import save2pickle

"""# CLASSIFIER & HYPERPARAMETERS"""
def find_best_params_withmodel(classifier, grid, X_train, y_train, X_test, y_test):
  #grid search to find the best hyperparameters for the classifier
  print("[INFO] grid searching over the hyperparameters...")
  cvFold = KFold(n_splits=5)
  randomSearch = GridSearchCV(estimator=classifier, n_jobs=(os.cpu_count())//2,
    cv=cvFold, param_grid=grid,
    scoring="f1_macro")
  
  print("Fitting the model")
  searchResults = randomSearch.fit(X_train, y_train)
  
  # extract the best model and evaluate it
  print("[INFO] evaluating...")
  bestModel = searchResults.best_estimator_
  bestScore = bestModel.score(X_test, y_test)
  print("Score with the best model: {:.2f}".format(bestScore))
  print("Best model params: {}".format(searchResults.best_params_))

  return bestModel, searchResults.best_params_, bestScore

def get_linearsvc_classifier(hyperparams=None):
  #linear svm function
  if hyperparams != None:
    classifier = LinearSVC(**hyperparams)
  else:
    classifier = LinearSVC()
  return classifier


def get_model_scores(classifier, X_train, y_train, X_test, y_test, vec, findbest = False, grid = None, showplot = False):
  #train model and return results
  X_train = vec.fit_transform(X_train).toarray()
  X_test = vec.transform(X_test).toarray()

  classifier.fit(X_train, y_train)

  # Reference: https://pyimagesearch.com/2021/05/17/introduction-to-hyperparameter-tuning-with-scikit-learn-and-python/
  # initialize a cross-validation fold and perform a Grid-search
  # to tune the hyperparameters
  if findbest:
    bestModel, best_params, best_score = find_best_params_withmodel(classifier, grid, X_train, y_train, X_test, y_test)
    classifier = bestModel

  Y_pred = classifier.predict(X_test)
  scores, macro_score = calculate_scores(y_test, Y_pred, showplot = showplot)

  return classifier, scores, macro_score, vec


"""# ARG PARSER"""
def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_file", default='../../data/train.tsv', type=str,
                        help="Training file")
    parser.add_argument("--dev_file", default='../../data/dev.tsv', type=str,
                        help="Validation file")
    parser.add_argument("--find_bestparams", action="store_true", default = False,
                        help="Find the best hyperparameters: True, False")
    parser.add_argument("--show_plot", action="store_false", default = False,
                        help="Show the confusion matrix plot.")
    parser.add_argument("--vec", type=str, default="ngram_count",
                        help="You can choose the type of vectorizer:\
                              tfidf, countvec, bothvec, ngram_bothvec, ngram_count, ngram_tfidf, pos")
    parser.add_argument("--C", type=float, default=0.01,
                        help="Margin of SVM")
    parser.add_argument("--class_weight", type=str, default="balanced",
                        help="To decide whether to give weightage to the less occuring classes or not")
    parser.add_argument("--output_modelname", type=str, default="best_model.pt",
                        help="To decide whether to give weightage to the less occuring classes or not")
    args = parser.parse_args()
    return args

"""# MODEL"""
def main():
  args = create_arg_parser()
  print(args)
  random.seed(64)

  #get data
  df_train = read_corpus(args.train_file)
  df_dev = read_corpus(args.dev_file)

  #preprocess data
  df_train['text'] = df_train.text.apply(preprocessing)
  df_dev['text'] = df_dev.text.apply(preprocessing)

  #separate text and labels for training
  x_train, Y_train = df_train.text, df_train.label
  x_dev, Y_dev = df_dev.text, df_dev.label

  #set best hyperparameters according to the result of the grid search
  best_hyperparams = dict()
  best_hyperparams = {'C':args.C, 'class_weight': args.class_weight}

  vec = get_vectorizer(args.vec)
  if "ngram" in args.vec or "pos" in args.vec:
    X_train = [" ".join(doc) for doc in x_train] # For ngrams, input expected is a list of sentences not splitted sents.
    X_dev = [" ".join(doc) for doc in x_dev]
  else:
    X_train = x_train
    X_dev = x_dev

  classifier = get_linearsvc_classifier(hyperparams=best_hyperparams)
  grid = dict(C = [0.01, 0.1, 1], class_weight = [None, 'balanced'])
  classifier, scores, macro_score, vec = get_model_scores(classifier, X_train, Y_train, X_dev,
                                                      Y_dev, vec, findbest = args.find_bestparams,
                                                      grid = grid, showplot = args.show_plot)

  save2pickle(classifier, args.output_modelname)
  print(f"Model is saved to {args.output_modelname}")
  save2pickle([args.vec, vec], f"{args.output_modelname}.vec")
  print(f"Vectorizer is saved to {args.output_modelname}")


  #print results
  print(scores)
  print(f"macro Fscore is {macro_score}")
  print("="*40)

if __name__ == '__main__':
    main()