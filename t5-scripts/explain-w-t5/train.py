# -*- coding: utf-8 -*-
"""t5-explain-templatize-pytorch lightining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxFqEefa57Eut4WFAxv0HA723l6uDngC
"""

import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping 
from pytorch_lightning.callbacks import ModelCheckpoint

import argparse
import torch
from specific_utils import LitModel, LitOffData, TemplateHandler


def create_arg_parser():
    parser = argparse.ArgumentParser()
    
    parser.add_argument("-i", "--train_file", default='../../data/train.tsv', type=str,
                        help="Input file to learn from (default train.txt)")
    
    parser.add_argument("-d", "--dev_file", type=str, default='../../data/dev.tsv',
                        help="Separate dev set to read in (default dev.txt)")
    
    parser.add_argument("--learning_rate", default=1e-4, type=float,
                        help="Learning rate for the optimizer")

    parser.add_argument("--batch_size", default=8, type=int,
                        help="Batch size for training")

    parser.add_argument("--num_epochs", default=5, type=int,
                        help="Number of epochs for training")

    parser.add_argument("--max_seq_len", default=150, type=int,
                        help="Maximum length of input sequence after BPE")

    parser.add_argument("--langmodel_name", default="t5-base", type=str,
                        help="Name of the base pretrained language model")

    parser.add_argument("--offensive_lexicon",
                        default="../lexicon_words/final_offensive_lexicon.txt",
                        type=str, help="Path of the txt file containing offensive lexicon")

    parser.add_argument("--ckpt_folder", default="./t5explain-files/", type=str,
                        help="Name of the checkpoint folder for saving the model")

    parser.add_argument("--seed", default=1234, type=int,
                        help="Seed for reproducible results")

    parser.add_argument("--device", default="gpu", type=str,
                        help="Type of device to use. gpu/cpu strict naming convention")

    args = parser.parse_args()
    return args



args = create_arg_parser()
print(args)

pl.seed_everything(args.seed, workers=True)

ckpt_folder = args.ckpt_folder

templatehandler = TemplateHandler()

print("Templates used are as follows:")
print("="*30)
print(f"label mapper => {templatehandler.labelmapper}")
print(f"Prompt added => {templatehandler.explanation_filler}")
print("="*30)

dm = LitOffData( templatehandler = templatehandler,
                 train_file =  args.train_file,
                 dev_file =  args.dev_file, 
                 offensive_lexfile=args.offensive_lexicon,
                 batch_size = args.batch_size,
                 max_seq_len = args.max_seq_len,
                 modelname = args.langmodel_name,)

model = LitModel(
                 templatehandler = templatehandler,
                 modelname = args.langmodel_name, 
                 learning_rate = args.learning_rate,
                 batch_size = args.batch_size)

#
early_stopping = EarlyStopping(monitor="val_loss", mode="min", patience = 3)
checkpoint_callback = ModelCheckpoint(dirpath=ckpt_folder,
                                      monitor="val_loss", mode="min",
                                      filename="best-model")

device_to_train = args.device if torch.cuda.is_available() else "cpu"

trainer = pl.Trainer(deterministic=True, accelerator=device_to_train, devices=1, 
                     max_epochs = args.num_epochs, fast_dev_run=None,
                     callbacks=[ early_stopping, checkpoint_callback ]
                     , default_root_dir = args.ckpt_folder)

trainer.fit(model, dm)

print(f"Best checkpoint is {checkpoint_callback.best_model_path}")
